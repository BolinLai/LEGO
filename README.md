# LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning

### [Project Page](https://bolinlai.github.io/Lego_EgoActGen/) | [Paper](https://arxiv.org/pdf/2312.03849) | [Dataset](https://www.dropbox.com/scl/fo/4m0v9oy753aimas8rz6v1/ANoJhZQz2BdcGIVLzUsHdP0?rlkey=o8saklcszfc098mjnpid767ic&dl=0)

### <font color=red>**Our dataset has been released!**</font>

Thank you for your interest in our work! The first version of the code has been released. We are editing the README instructions.

ToDo:

- [x] Dataset

- [x] Codes

- [ ] README (Updating...)

- [ ] Checkpoints

 <img src='https://bolinlai.github.io/Lego_EgoActGen/figures/visualization_new_actions.png'/>



## Contents
- [Setup](#setup)
- [Dataset](#dataset)
- [GPT-Curated Data (Optional)](#gpt-curated-data-optional)
- [VLLM Features](#vllm-features)
- [Model Weights](#model-weights)
- [Train and Inference](#train-and-inference)
  - [VLLM Training](#vllm-training)
  - [VLLM Inference](#vllm-inference)
  - [LDM Training](#ldm-training)
  - [LDM Inference](#ldm-inference)
- [Metrics](#metrics)
- [BibTex](#bibtex)



## Setup

Due to the incompatibility of VLLM and LDM packages, we use two environments for each model.

Install the dependencies for VLLM.
dd
```shell
conda env create -f vllm_env.yaml  # The env name is "vllm".
conda activate vllm
pip install flash-attn==2.0.7 --no-build-isolation
```

Install the dependencies for LDM.

```shell
conda env create -f ldm_env.yaml  # The env name is "ldm".
```


## Dataset

Run this command to download the dataset.
```shell
bash scripts/download_dataset.sh [your_local_path]  # replace [your_local_path] to your local download path.
```
You can also download the dataset from this [link](https://www.dropbox.com/scl/fo/4m0v9oy753aimas8rz6v1/ANoJhZQz2BdcGIVLzUsHdP0?rlkey=o8saklcszfc098mjnpid767ic&dl=0). Our dataset is composed of video frames (in `EgoGen.zip`) and action instructions (in `*.json`) from Ego4D and Epic-Kitchens.  Unzip `EgoGen.zip` to your local path. 

The structure of the dataset is as follows. Note that `val_for_metric` only contains ground truth images of `val`. It's simply used for metric calculation and not invovled in training and inference.
```
[your_local_path]
        |
        |-- EgoGen
        |     |
        |     |-- ego4d.fho
        |     |       |-- train
        |     |       |-- val
        |     |       |-- val_for_metric
        |     |
        |     |-- epickitchen
        |             |-- train
        |             |-- val
        |             |-- val_for_metric
        |
        |-- ego4d_train.json
        |-- ego4d_val.json
        |-- epickitchen_train.json
        |-- epickitchen_val.json
    
```
The test sets of Ego4d and Epic-Kitchens are hidden so we use the validation set as test set in our experiments. In `*.json`,`image_0` and `image_1` denote the source and target images for action frame generation. `action` refers to the original action labels/descriptions in the Ego4D and Epic-Kitchens. We also release the enriched action descriptions (`llava_forecast_finetune`) generated by LLM.


## GPT-Curated Data (Optional)

We released our detailed acton descriptions curated from GPT-3.5, which are used for VLLM instruction tuning. You can download by running the following command, or from this [link](https://www.dropbox.com/scl/fo/2iez9r5k5z9f0azxchb09/AKC_p3Y96wHpDXCOgOYnZB0?rlkey=ndh4vzedl3w66b4r90kzzz5s7&st=ozy2upb1&dl=0).
```shell
bash scripts/download_gpt_curated_data.sh
```
**Note: This step is only necessary if you want to finetune the VLLM component in our model. Otherwise, you can directly use the released VLLM weights for inference.**


## VLLM Features

In addition, we also release the VLLM image and text features, and then you can train the LDM component without running VLLM inference. You can download and unzip all features by running this command. 

Please make sure there are 500GB available on your machine before downloading.
```shell
bash scripts/download_vllm_features.sh [your_local_path]  # replace [your_local_path] to your local download path.
```
You can also download the features from [here](https://www.dropbox.com/scl/fo/5tr0fjzjxzimnvgx6tath/AGQZnNWt8dHERcyBPzTdIGQ?rlkey=1gqx4p0v4pf869q225rwynt3r&dl=0). Then, unzip each zip file in the same folder by using `unzip xxx.zip`. For ego4d image features, we split the data in `.zip` and `.z01` files. You can put them in the same folder and run `zip -s0 llava-llama-2-13b-chat-forecasting-finetune-ckpt450-split.zip --out merge.zip` to merge them to one single zip file. Then you can unzip `merge.zip`.


## Model Weights

Download the pretrained LLM models by running:
```shell

```

Download the checkpoints for computing metrics via this link.


## Train and Inference

We train the VLLM and LDM components separately. Both of them are trained on 8x40G A100.

### VLLM Training

Activate `vllm` virtual environment.

```shell
conda activate vllm
```

**Train VLLM on Ego4D**

Before running the script, you have to update a bunch of paths in `vllm/scripts/finetune_ego4d.sh` to your local paths where you download the data and checkpoints in above steps.

`--model_name_or_path`: The path of pretrained VLLM checkpoint.

`--data_path`: The path of detailed action descriptions curated from GPT-3.5.

`--image_folder`: The path of Ego4D training data (i.e., video frames).

`--output_dir`: The path to save checkpoints. 

Then run the command below.

```shell
bash vllm/scripts/finetune_ego4d.sh
```

**Train VLLM on Epic-Kitchens** 

Similarly, you need to update the same paths in `vllm/scripts/finetune_epickitchen.sh` as listed above.

```shell
bash vllm/scripts/finetune_epickitchen.sh
```

### VLLM Inference

Activate `vllm` virtual environment.

```shell
conda activate vllm
```

Run VLLM inference on Ego4D or Epic-Kitchens. To speed up inference, we divide the data into five groups and run inference on them separately.

(1) Use Slurm

If you are using slurm to launch jobs, before running the script, you have to update the paths in `vllm/scripts/sbatch_inference.sh` to your local paths.

`model_path`: The path of instruction-tuned VLLM weights.

`image_dir`: The path of video frames.

`action_label`: The path of action labels (i.e., *.json files downloaded with video frames).

`save_path`: The path to save generated enriched action descriptions.

`save_image_feature_path`: The path to save VLLM image features.

`save_text_feature_path`: The path to save VLLM text features.

Then run the command below.

```shell
bash vllm/scripts/sbatch_inference.sh
```

Then merge the output in one json file using

```python
python vllm/scripts/merge_inference_results.py
```

(2) Without Slurm

Run the inference code directly. You can use `--num-chunks` to control how many chunks the data will be divided into, and `--chunk-idx` to specify which chunk to be used for inference. The paths should be changed to your local paths as elaborated above.

```shell
export PYTHONPATH=$PYTHONPATH:./vllm

python -m llava.eval.run_llava_in_loop \
    --model-path /fsx-project/bolinlai/Release/checkpoints/VLLM/ego4d/llava-llama-2-13b-chat-forecasting-finetune \
    --image-dir /fsx-project/bolinlai/Release/dataset/EgoGen/ego4d.fho/val \
    --action-label /fsx-project/bolinlai/Release/dataset/ego4d_val.json \
    --query "How does the person properly {} that is displayed in the video frame?" \
    --save-path /fsx-project/bolinlai/Release/vllm_output/ego4d/llava-llama-2-13b-chat-forecasting-finetune/val \
    --save-image-feature-path /fsx-project/bolinlai/Release/vllm_features/ego4d/vllm_image_features/llava-llama-2-13b-chat-forecasting-finetune/val \
    --save-text-feature-path /fsx-project/bolinlai/Release/vllm_features/ego4d/vllm_text_features/llava-llama-2-13b-chat-forecasting-finetune/val \
    --seed 42 \
    --num-chunks 5 \
    --chunk-idx 1
```

Then merge the output in one json file using

```python
python vllm/scripts/merge_inference_results.py
```


### LDM Training

Activate `ldm` virtual environment.

```shell
conda activate ldm
```

**Train LDM on Ego4D** 

Before launching the job, you have to update the paths in `configs/train_ego4d.yaml`.

`model.params.ckpt_path`: The path of pretrained latent diffusion model weights.

`data.params.train.params.data_path`: The path of Ego4D video frames in training set.

`data.params.train.params.data_path`: The path of Ego4D action descriptions in training set (i.e., `ego4d_train.json` downloaded with video frames).

`data.params.train.params.additional_cond_path`: The paths of VLLM image and text features in training set.

`data.params.validation.params.data_path`: The path of Ego4D video frames in val set.

`data.params.validation.params.data_path`: The path of Ego4D action descriptions in val set (i.e., `ego4d_val.json` downloaded with video frames).

`data.params.validation.params.additional_cond_path`: The paths of VLLM image and text features in val set.

Then run the command below.

```shell
python main.py --name lego_ego4d --base configs/train_ego4d.yaml --train --gpus 0,1,2,3,4,5,6,7
```

**Train LDM on Epic-Kitchens**

Similarly, update the corresponding paths in `configs/train_kitchen.yaml` as listed above. Then run the following command.

```shell
python main.py --name lego_epickitchens --base configs/train_kitchen.yaml --train --gpus 0,1,2,3,4,5,6,7
```

### LDM Inference

Activate `ldm` virtual environment.

```shell
conda activate ldm
```

To speed up inference, we divide the data into eight groups and run inference on them separately. Similar to training, you need to update the paths in `configs/generate_ego4d.yaml` and `configs/generate_kitchen.yaml` for Ego4D and Epic-Kitchens inference, respectively.

**Run LDM inference on Ego4D**

(1) Use Slurm

```
bash test_ego4d.sh logs/train_ego4d_llava_img_posttxt2sa/checkpoints/trainstep_checkpoints/epoch\=000130-step\=000010999.ckpt
```

**Run LDM inference on Epic-Kitchens**

(1) Use Slurm

```
bash test_epickitchen.sh logs/train_ego4d_llava_img_posttxt2sa/checkpoints/trainstep_checkpoints/epoch\=000130-step\=000010999.ckpt
```

## BibTex

If you find LEGO useful for your work, please cite using this BibTex.

```
@article{lai2023lego,
        title={LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning},
        author={Lai, Bolin and Dai, Xiaoliang and Chen, Lawrence and Pang, Guan and Rehg, James M and Liu, Miao},
        journal={arXiv preprint arXiv:2312.03849},
        year={2023}
      }
```


## Acknowledgement

Our code was built on [LLaVA](https://github.com/haotian-liu/LLaVA) and [InstructPix2Pix](https://github.com/timothybrooks/instruct-pix2pix). We appreciate the authors of the two awesome codebases.
