# LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning

### [Project Page](https://bolinlai.github.io/Lego_EgoActGen/) | [Paper](https://arxiv.org/pdf/2312.03849) | [Dataset](https://www.dropbox.com/scl/fo/4m0v9oy753aimas8rz6v1/ANoJhZQz2BdcGIVLzUsHdP0?rlkey=o8saklcszfc098mjnpid767ic&dl=0)

### <font color=red>**Our dataset has been released!**</font>

Thank you for your interest in our work! The first version of the code has been released. We are editing the README instructions.

ToDo:

- [x] Dataset

- [x] Codes

- [ ] README (Updating...)

- [ ] Checkpoints

 <img src='https://bolinlai.github.io/Lego_EgoActGen/figures/visualization_new_actions.png'/>



## Contents
- [Setup](#setup)
- [Dataset](#dataset)
- [Model Weights](#model-weights)
- [Train](#train)
- [Evaluation](#evaluation)

## Setup

Due to the incompatibility of VLLM and LDM packages, we use two environments for each model.

Install the dependencies for VLLM.

```shell
conda env create -f vllm_env.yaml  # The env name is "vllm".
conda activate vllm
pip install flash-attn==2.0.7 --no-build-isolation
```

Install the dependencies for LDM.

```shell
conda env create -f ldm_env.yaml  # The env name is "ldm".
```

## Dataset

Download the dataset from this [link](https://www.dropbox.com/scl/fo/4m0v9oy753aimas8rz6v1/ANoJhZQz2BdcGIVLzUsHdP0?rlkey=o8saklcszfc098mjnpid767ic&dl=0). Our dataset is composed of video frames (in `EgoGen.zip`) and action instructions (in `*.json`) from Ego4D and Epic-Kitchens. 

Unzip `EgoGen.zip` to your local path. The structure of the dataset is like
```
EgoGen
  |-- ego4d.fho
  |     |-- train
  |     |-- val
  |
  |-- epickitchen
  |     |-- train
  |     |-- val
  |
  |-- ego4d_train.json
  |-- ego4d_val.json
  |-- epickitchen_train.json
  |-- epickitchen_val.json
    
```
The test sets of ego4d and epickitchens are hidden so we use the validation set as test set in our experiments. In `*.json`,`image_0` and `image_1` denote the source and target images for action frame generation. `action` refers to the original action labels/descriptions in the Ego4D and Epic-Kitchens. We also release the detailed action descriptions (`llava_forecast_finetune`) generated by LLM.

In addition, we also release the VLLM image and text features [here](https://www.dropbox.com/scl/fo/5tr0fjzjxzimnvgx6tath/AGQZnNWt8dHERcyBPzTdIGQ?rlkey=1gqx4p0v4pf869q225rwynt3r&dl=0). Unzip the features in each folder. For ego4d image features, we split the data in `.zip` and `.z01` files. You can put them in the same folder and directly unzip them using `unzip xxx.zip`. The tool will automatically merge the two zip files.


## Model Weights

Download the pretrained LLM models by running:
```shell

```

Download the checkpoints for computing metrics via this link.


## Train

Our model is trained on 8x40G GPUs.

Train VLLM on Ego4D.

```shell
bash scripts/finetune_ego4d.sh
```

Train VLLM on Epic-Kitchens.

```shell
bash scripts/finetune_epickitchen.sh
```

Train LDM on Ego4D.

```shell
python main.py --name lego_ego4d --base configs/train_ego4d.yaml --train --gpus 0,1,2,3,4,5,6,7
```

Train LDM on Epic-Kitchens.

```shell
python main.py --name lego_epickitchens --base configs/train_kitchen.yaml --train --gpus 0,1,2,3,4,5,6,7
```


## Evaluation


## BibTex

If you find LEGO useful for your work, please cite using this BibTex.

```
@article{lai2023lego,
        title={LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning},
        author={Lai, Bolin and Dai, Xiaoliang and Chen, Lawrence and Pang, Guan and Rehg, James M and Liu, Miao},
        journal={arXiv preprint arXiv:2312.03849},
        year={2023}
      }
```


## Acknowledgement

Our code was built on [LLaVA](https://github.com/haotian-liu/LLaVA) and [InstructPix2Pix](https://github.com/timothybrooks/instruct-pix2pix). We appreciate the authors of the two awesome codebases.
